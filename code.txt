# 1.model preprocess --------------------------------------------------------------
# Load necessary libraries
library(xgboost)
library(Boruta)  # Feature selection with the Boruta algorithm
library(caret)
library(pROC)
library(ggplot2)
library(e1071)
library(MASS)
library(glm2)
library(glmnet)

# Preparing data --------------------------------------------------------------
# Data loading
dat0 = read.csv("res.csv", head = TRUE, row.names = 1)

# Data Preprocessing and Splitting #
# Handling missing values
print(nrow(dat0))
dat = na.omit(dat0)
print(nrow(dat))

# Processing all data
# Removing near-zero variance
print(dim(dat[, 1:3751]))
nzv = nearZeroVar(dat[, 1:3751])
print(dim(nzv))
dat_1 = dat[, -nzv]

# Removing collinear variables
df11 = dat_1[, 1:2810]
descrCor = cor(df11)
print(descrCor)
highlyCorDescr = findCorrelation(descrCor, cutoff = 0.75, names = FALSE, verbose = TRUE)
filtereddat = dat_1[, -highlyCorDescr]
dat_2 = filtereddat

# Converting 'Class' to a factor
dat_2$Class = factor(dat_2$Class, levels = c(0, 1), labels = c("NC", "C"))

# Variable Selection
# Recursive feature selection
set.seed(7)
subsets = c(5, 10, 15, 20, 30, 40, 50, 60, 70, 80, 90, 100)
ctrl = rfeControl(functions = rfFuncs, method = "cv", number = 10)
x = dat_2[, 1:1334]
y = dat_2[, "Class"]
profile = rfe(x, y, sizes = subsets, rfeControl = ctrl)
print(profile$optVariables)  # Selected variables
plot(profile)

# Selecting dataframe with significant variables
selected_var = profile$optVariables
dat_3 = dat_2[, c('SsCH3', 'GOBP_REGULATION_OF_SODIUM_ION_TRANSMEMBRANE_TRANSPORT',
                 'SpMax3_Bhp', 'ATSC6i', 'GOBP_NEGATIVE_REGULATION_OF_BLOOD_VESSEL_ENDOTHELIAL_CELL_MIGRATION',
                 'ATSC3i', 'ATSC1i', 'ATSC3p', 'SHBint7', "Class")]
                 
# Data normalization
dat_4 <- data.frame(scale(dat_3[,1:9]))
dat_4$Class <- dat_3$Class

# Data Partitioning using Holdout Method #
# Set random seed
set.seed(5)
# Splitting the dataset into 80% training and 20% test
trainIndex = createDataPartition(dat_3$Class, p = 0.8, list = FALSE, times = 1)
datTrain = dat_3[trainIndex, ]
datTest = dat_3[-trainIndex, ]
datTrain.svm = dat_4[trainIndex, ]
datTest.svm = dat_4[-trainIndex, ]
datTrain.IDA = dat_4[trainIndex, ]
datTest.IDA = dat_4[-trainIndex, ]

# Checking proportions of dependent variable
print(table(dat$Class) / nrow(dat))
print(table(datTrain$Class) / nrow(datTrain))
print(table(datTest$Class) / nrow(datTest))


# modeling --------------------------------------------------------------------

# XGBoost_model
# Dummy variable creation
dvfunc = dummyVars(~., data = datTrain[, 1:9], fullRank = TRUE)
dat_trainx = predict(dvfunc, newdata = datTrain[, 1:9])
dat_trainy = ifelse(datTrain$Class == "NC", 0, 1)
dat_testx = predict(dvfunc, newdata = datTest[, 1:9])
dat_testy = ifelse(datTest$Class == "NC", 0, 1)
dtrain = xgb.DMatrix(data = dat_trainx, label = dat_trainy)
dtest = xgb.DMatrix(data = dat_testx, label = dat_testy)
# Creating a parameter grid for tuning
grid = expand.grid(
  nrounds = c(10,50,100,150,200,250,300,350,400,450,500,550,600), # Reduced the number of values to save time
  eta = c(0.01, 0.05, 0.1), # Offering different learning rates for testing
  colsample_bytree = c(0.5, 0.75, 1), # Trying different column sampling ratios
  min_child_weight = c(1, 3, 5), # Trying different minimum child node weights
  gamma = c(0, 0.1, 0.2), # Trying different gamma values
  subsample = c(0.5, 0.75, 1), # Trying different subsample ratios
  max_depth = c(4, 6, 8) # Trying different maximum depths
)
# Control parameters for training
cntrl = trainControl(
  method = "cv",
  number = 10,
  verboseIter = TRUE,
  returnData = FALSE,
  returnResamp = "final"
)
set.seed(1234)
train.xgb = train(
  x = datTrain[, 1:9],
  y = datTrain[, 10],
  trControl = cntrl,
  tuneGrid = grid,
  method = "xgbTree",
  objective = "binary:logistic"
)
# Parameters for xgb
param = list(
  objective = "binary:logistic",
  booster = "gbtree",
  eval_metric = "auc",
  eta = 0.01,
  max_depth = 4,
  subsample = 0.6,
  colsample_bytree = 0.6,
  gamma = 0,
  scale_pos_weight = 9,
  min_child_weight = 1
)
set.seed(1234)
xgb_fit = xgb.train(params = param, data = dtrain, nrounds = 150)

# SVM_model 
set.seed(123)
rbf.tune <- tune.svm(Class ~ ., data = datTrain.svm, 
                      kernel = "radial",
                      gamma = c(0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1),
                      cost = c(0.1,0.5,1,2,3,4,5,6,7,8,9,10,100),
                      probability = T,
                      tunecontrol = tune.control(sampling = "cross", 
                                                 cross = 10, 
                                                 best.model = T, 
                                                 performances = T))
# The proportion of each category of the model is determined to be 9:1
wts=c(9,1)
names(wts)=c("NC","C")
set.seed(123)
rbf.svm= svm(Class~ ., 
              data = datTrain.svm,
              cost = 7, 
              gamma = 0.3,
              kernel = "radial",
              probability = TRUE,scale = F,class.weights = wts)

# LR_model
set.seed(123)
LR = train(Class ~., data=datTrain, method="glm", family = "binomial", 
                 trControl = trainControl(method = "cv", number = 10))
                            
# LDA_model
LDA =lda(Class~., data=datTrain.IDA)

# NB_model
set.seed(123)
bayes <- naiveBayes(Class ~ ., datTrain,laplace = 1)

# Predicting ------------------------------------------------------------------

# Prediction on training set
data.Trainpredictprob = predict(model_fit, newdata = Train_set, type = "prob")

# Training set ROC
trainroc = roc(response = datTrain$Class, predictor = data.Trainpredictprob)
plot(trainroc, print.auc = TRUE, grid = TRUE, max.auc.polygon = TRUE, auc.polygon.col = "skyblue", print.thres = TRUE, legacy.axes = TRUE, bty = "l")

# bestp index
bestp = trainroc$thresholds[which.max(trainroc$sensitivities + trainroc$specificities - 1)]

# Training set prediction classification
Trainpredlab = factor(ifelse(data.Trainpredictprob > bestp, "C", "NC"))
confusionMatrix(Trainpredlab, Train_set$Class, mode = "everything")

# Prediction on test set
data.Testpredictprob = predict(model_fit, newdata = Test_set, type = "prob")

# Test set ROC
testroc = roc(response = datTest$Class, predictor = data.Testpredictprob)
plot(testroc, print.auc = TRUE, grid = TRUE, max.auc.polygon = TRUE, auc.polygon.col = "skyblue", print.thres = TRUE, legacy.axes = TRUE, bty = "l")

# Test set prediction classification
Testpredlab = factor(ifelse(data.Testpredictprob > bestp, "C", "NC"))
confusionMatrix(Testpredlab, Test_set$Class, mode = "everything")


# ensemble model---------------------------------------------------------------

# Stacking model
# Prediction probability results of five models: 
XG_probs, LDA_probs, SVM_probs, LR_probs, NB_probs
# Assembling stacked training and testing datasets
stacking_train <- data.frame(XGBoost = XG_train_probs, LDA = LDA_train_probs, SVM = SVM_train_probs, LR = LR_train_probs, NB = NB_train_probs, True_Labels = train$Class)
stacking_test <- data.frame(XGBoost = XG_test_probs, LDA = LDA_test_probs, SVM = SVM_test_probs, LR = LR_test_probs, NB = NB_test_probs, True_Labels = test$Class)

# Setting tuning grid parameters
ctrl <- trainControl(method = "cv", number = 5)
set.seed(1234)
glm_grid <- expand.grid(alpha = seq(0, 1, by = 0.1), lambda = seq(0, 1, by = 0.1))
colnames(glm_grid) <- c("alpha", "lambda")

# Training the stacking model with parameter tuning
set.seed(1234)
stack_model <- train(True_Labels ~ ., data = stacking_train, method = "glmnet", 
                     trControl = ctrl, tuneGrid = glm_grid)

# Predictions on training set
predictions_train <- predict(stack_model, newdata = stacking_train)
stacking_train$True_Labels <- factor(stacking_train$True_Labels, levels = c("C", "NC"))

# Predictions on test set
predictions_test <- predict(stack_model, newdata = stacking_test)
stacking_test$True_Labels <- factor(stacking_test$True_Labels, levels = c("C", "NC"))

# Confusion matrix for train set
confusionMatrix(predictions_train, stacking_train$True_Labels, mode = "everything")

# Confusion matrix for test set
confusionMatrix(predictions_test, stacking_test$True_Labels, mode = "everything")

# Stacking_W model
# Predictions on training set
predictions_train <- predict(stack_model, newdata = stacking_train)
stacking_train$True_Labels <- factor(stacking_train$True_Labels, levels = c("C", "NC"))
predictions_train_prob <- predict(stack_model, newdata = stacking_train, type = "prob")

# Predictions on test set
predictions_test <- predict(stack_model, newdata = stacking_test)
stacking_test$True_Labels <- factor(stacking_test$True_Labels, levels = c("C", "NC"))
predictions_test_prob <- predict(stack_model, newdata = stacking_test, type = "prob")

# Model evaluation metrics for training set
# ROC for training set
trainroc <- roc(response = stacking_train$True_Labels, 
                predictor = predictions_train_prob[, 1])
# Plot ROC for training set
plot(trainroc, print.auc = TRUE, auc.polygon = TRUE, grid = TRUE, 
     max.auc.polygon = TRUE, auc.polygon.col = "skyblue", print.thres = TRUE, 
     legacy.axes = TRUE, bty = "l")

# Determine the best threshold (Youden's J statistic)
bestp <- trainroc$thresholds[which.max(trainroc$sensitivities + trainroc$specificities - 1)]

# Predictions on training set based on best threshold
trainpredlab <- factor(ifelse(predictions_train_prob[, 1] > bestp, "C", "NC"))

# Confusion matrix for training set
confusionMatrix

# Predictions on test set
testpredlab = factor( 
  ifelse(predictions_test_prob[, 1] > bestp, "C", "NC")
)

# Confusion matrix for test set
confusionMatrix(testpredlab, stacking_test$True_Labels,mode = "everything")

# 2.FGESA -----------------------------------------------------------------------
# Load necessary libraries
library(BiocManager)
library(clusterProfiler)
library(fgsea)

geneset1 <- read.gmt("c5.go.bp.v7.5.1.symbols.gmt") 
geneset2 <- read.gmt("c2.cp.reactome.v7.5.1.symbols.gmt") 
geneset3 <- read.gmt("c2.cp.kegg.v7.5.1.symbols.gmt")
geneset4 <- read.gmt("c2.cp.wikipathways.v7.5.1.symbols.gmt") 
geneset5 <- read.gmt("Hallmark_CTD_geneset1.gmt") 
geneset6 <- read.gmt("Hallmark_CTD_geneset2.gmt")
geneset10 <- gmtPathways("all.gmt")

# Batch reading CSV files
mycsvfile <- dir(path = "directory", pattern = "csv")

# Reading and converting CSV files into data frames in the global environment
list2env(
  lapply(setNames(mycsvfile, make.names(gsub("\\.csv$", "", mycsvfile))),
         read.csv, header = TRUE, check.names = FALSE), 
  envir = .GlobalEnv
)

# Looping through each data frame for processing
for (csv in mycsvfile) {
  # Reading CSV file
  df <- read.csv(csv, header = TRUE, check.names = FALSE)
  
  # Vectorization
  df_vector <- df$weight
  names(df_vector) <- df$results

  # Enrichment analysis using fgsea
  library(fgsea)
  fgseaResMultilevel <- fgsea(geneset10, df_vector, scoreType = "pos",
                              nproc = 0, gseaParam = 1, BPPARAM = NULL,
                              nPermSimple = 1000, absEps = NULL)
  
  # Converting to matrix and saving the results
  gsea <- as.matrix(fgseaResMultilevel)
  output_filename <- paste0("fgsea_", gsub("\\.csv$", "", csv), ".csv")
  write.csv(gsea, file = output_filename)

  # Clear memory
  gc()
}

# 3.STRINGdb -----------------------------------------------------------------------
# Load necessary libraries
library(tidyverse)
library(clusterProfiler)
library(org.Mm.eg.db)
library(bioconductor)
library(STRINGdb)
library(igraph)
library(ggraph)

# Create a STRINGdb object
string_db <- STRINGdb$new(version = "11", species = 9606, score_threshold = 400, 
                          input_directory = "")

# Read data from CSV file
df001 <- read.csv("placental-gene.csv", header = TRUE)

# Map gene IDs to the STRING database
data_mapped <- string_db$map(df001, "SYMBOL", removeUnmappedRows = TRUE)

# Display the number of IDs mapped
cat("Total String id mapped:", dim(data_mapped)[1], "\n")

# Set plot parameters
options(SweaveHooks = list(fig = function() par(mar = c(2.1, 0.1, 4.1, 2.1))))

# Select a subset of results for plotting
hits <- data_mapped

# Plot the network
string_db$plot_network(hits, required_score = 400)

# Export all results to a file for network analysis with Cytoscape
info <- data_mapped$STRING_id %>% string_db$get_interactions()
write.table(info, file = "STRING_info.txt", sep = "\t", row.names = FALSE, quote = FALSE)

# Convert STRING IDs to Symbols and select relevant columns
links <- info %>%
  mutate(from = data_mapped[match(from, data_mapped$STRING_id), "SYMBOL"]) %>%
  mutate(to = data_mapped[match(to, data_mapped$STRING_id), "SYMBOL"]) %>%
  dplyr::select(from, to, last_col()) %>%
  dplyr::rename(weight = combined_score)

# Remove isolated interaction relationships
links_2 <- links %>%
  mutate(from_c = count(., from)$n[match(from, count(., from)$from)]) %>%
  mutate(to_c = count(., to)$n[match(to, count(., to)$to)]) %>%
  filter(!(from_c == 1 & to_c == 1)) %>%
  dplyr::select(1, 2, 3)

# Write the filtered links to a file
write.table(links_2, file = "links_2.txt", sep = "\t", row.names = FALSE, quote = FALSE)
